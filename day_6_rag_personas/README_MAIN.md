# ğŸ¤– RAG Implementation - Day 6

Complete RAG (Retrieval Augmented Generation) system using LlamaIndex, LanceDB, and Ollama.

---

## ğŸ“ Folder Structure

```
day_6/
â”œâ”€â”€ RAG_Implementation.py          # Main implementation (run this first!)
â”‚
â”œâ”€â”€ ui_apps/                       # Web interfaces
â”‚   â”œâ”€â”€ app.py                     # Streamlit UI (recommended)
â”‚   â””â”€â”€ app_gradio.py              # Gradio UI (alternative)
â”‚
â”œâ”€â”€ testing_tools/                 # Testing utilities
â”‚   â”œâ”€â”€ test_rag.py                # Interactive terminal testing
â”‚   â”œâ”€â”€ quick_test.py              # Quick command-line queries
â”‚   â””â”€â”€ diagnose_llm.py            # System diagnostics
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ README.md                  # Testing guide
â”‚   â”œâ”€â”€ CODE_WALKTHROUGH.md        # Detailed code explanation
â”‚   â”œâ”€â”€ UI_GUIDE.md                # UI usage instructions
â”‚   â”œâ”€â”€ CHROME_TROUBLESHOOTING.md  # Browser troubleshooting
â”‚   â”œâ”€â”€ LLM_STATUS.md              # LLM integration status
â”‚   â””â”€â”€ Day6-RAG.md                # Original notes
â”‚
â”œâ”€â”€ scripts/                       # Helper scripts
â”‚   â”œâ”€â”€ demo.sh                    # Quick demo
â”‚   â”œâ”€â”€ launch_ui.sh               # UI launcher
â”‚   â””â”€â”€ open_ui.html               # Access page
â”‚
â”œâ”€â”€ data/                          # Source documents (100 persona files)
â”œâ”€â”€ lancedb_data/                  # Vector database
â””â”€â”€ .streamlit/                    # Streamlit configuration
    â””â”€â”€ config.toml
```

---

## ğŸš€ Quick Start

### 1. First Time Setup
```bash
cd day_6

# Run the main implementation
python RAG_Implementation.py
```

This will:
- âœ… Load 100 personas from HuggingFace
- âœ… Create vector embeddings
- âœ… Store in LanceDB
- âœ… Test the system
- â±ï¸ Takes ~35 seconds

---

### 2. Choose Your Interface

#### Option A: Streamlit UI (Recommended)
```bash
cd day_6
streamlit run ui_apps/app.py
```
Then open: http://localhost:8501

#### Option B: Gradio UI
```bash
cd day_6
python ui_apps/app_gradio.py
```
Then open: http://localhost:7860

#### Option C: Terminal
```bash
cd day_6

# Interactive mode
python testing_tools/test_rag.py

# Quick query
python testing_tools/quick_test.py "Who are the AI experts?"
```

---

## ğŸ”§ System Requirements

### Installed:
- âœ… Python 3.8+
- âœ… Ollama with gemma3:1b model
- âœ… All Python dependencies

### Check Setup:
```bash
cd day_6
python testing_tools/diagnose_llm.py
```

---

## ğŸ“š Documentation

All documentation is in `docs/` folder:

| File | Description |
|------|-------------|
| `README.md` | Testing and usage guide |
| `CODE_WALKTHROUGH.md` | Line-by-line code explanation |
| `UI_GUIDE.md` | Web UI usage instructions |
| `CHROME_TROUBLESHOOTING.md` | Browser troubleshooting |
| `LLM_STATUS.md` | LLM integration details |

**Read the walkthrough:**
```bash
cat docs/CODE_WALKTHROUGH.md
```

---

## ğŸ¯ Common Tasks

### Test the System
```bash
cd day_6
python testing_tools/diagnose_llm.py
```

### Ask a Question
```bash
cd day_6
python testing_tools/quick_test.py "your question here"
```

### Launch Web UI
```bash
cd day_6
streamlit run ui_apps/app.py
```

### Interactive Testing
```bash
cd day_6
python testing_tools/test_rag.py
```

---

## ğŸ¨ Three Search Modes

### 1. Vector Search Only (Fast)
- Speed: < 1 second
- Returns: Raw document chunks
- No LLM generation

### 2. RAG with LLM (Smart)
- Speed: 3-5 seconds
- Returns: Natural language answers
- Uses Ollama gemma3:1b locally

### 3. Both
- Shows both vector results and LLM response

---

## ğŸ’¾ Data Files

### Source Data (`data/`)
- 100 text files with persona descriptions
- Generated from HuggingFace dataset
- Can be regenerated by running main script

### Vector Database (`lancedb_data/`)
- Vector embeddings of all documents
- Search indices
- Can be regenerated by running main script

---

## ğŸ”„ Regenerate Data

If you need to start fresh:

```bash
cd day_6

# Remove existing data
rm -rf data/ lancedb_data/

# Regenerate everything
python RAG_Implementation.py
```

---

## ğŸ› ï¸ Troubleshooting

### Issue: "Module not found"
```bash
# Install dependencies
pip install llama-index llama-index-vector-stores-lancedb \
  llama-index-embeddings-huggingface llama-index-llms-ollama \
  lancedb datasets streamlit gradio
```

### Issue: "Ollama not found"
```bash
# Check Ollama
ollama --version
ollama list

# If gemma3:1b not found
ollama pull gemma3:1b
```

### Issue: "LanceDB data not found"
```bash
# Regenerate data
python RAG_Implementation.py
```

### Issue: UI not loading
```bash
# Check diagnostics
python testing_tools/diagnose_llm.py

# Use terminal instead
python testing_tools/test_rag.py
```

For more help, see: `docs/CHROME_TROUBLESHOOTING.md`

---

## ğŸ“– Learn More

### Understanding the Code
- Read: `docs/CODE_WALKTHROUGH.md`
- Explains every step of execution
- Visual diagrams and examples

### Using the Web UI
- Read: `docs/UI_GUIDE.md`
- Streamlit and Gradio instructions
- Tips and troubleshooting

### LLM Integration
- Read: `docs/LLM_STATUS.md`
- How the LLM integration works
- Testing and verification

---

## ğŸ“ Key Concepts

### RAG (Retrieval Augmented Generation)
1. **Retrieval**: Find relevant documents using vector search
2. **Augmented**: Add context to the question
3. **Generation**: LLM creates answer using context

### Vector Embeddings
- Convert text to numerical vectors
- Similar texts have similar vectors
- Enables semantic search

### LanceDB
- Vector database for fast similarity search
- Stores document embeddings
- Efficient retrieval

---

## âœ¨ Features

- âœ… 100 persona database
- âœ… Vector similarity search
- âœ… Local LLM with Ollama (gemma3:1b)
- âœ… Two web UIs (Streamlit + Gradio)
- âœ… Terminal interface
- âœ… Comprehensive testing tools
- âœ… Full documentation
- âœ… Diagnostic utilities

---

## ğŸš€ Next Steps

1. **Test the system:**
   ```bash
   python testing_tools/diagnose_llm.py
   ```

2. **Try the web UI:**
   ```bash
   streamlit run ui_apps/app.py
   ```

3. **Ask questions:**
   ```bash
   python testing_tools/quick_test.py "Who are the teachers?"
   ```

4. **Read the walkthrough:**
   ```bash
   cat docs/CODE_WALKTHROUGH.md
   ```

5. **Customize for your data:**
   - Modify `prepare_data()` function
   - Load your own documents
   - Run the pipeline!

---

## ğŸ“ Support

All tools are working! If you encounter issues:

1. Run diagnostics: `python testing_tools/diagnose_llm.py`
2. Check documentation in `docs/`
3. Try terminal interface: `python testing_tools/test_rag.py`

---

**Happy RAG building! ğŸ‰**




