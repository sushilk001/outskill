# LangChain Application - Complete Walkthrough

## ðŸ“š Table of Contents
1. [Setup and Architecture](#setup-and-architecture)
2. [Basic Chain Application](#basic-chain-application)
3. [Conversation with Memory](#conversation-with-memory)
4. [RAG (Document Q&A)](#rag-document-qa)
5. [Agent with Tools](#agent-with-tools)
6. [Web UI](#web-ui)

---

## 1. Setup and Architecture

### Project Architecture

```
LangChain Application
â”œâ”€â”€ Configuration Layer (config.py)
â”‚   â””â”€â”€ Centralized settings (API keys, models, parameters)
â”‚
â”œâ”€â”€ Core Applications
â”‚   â”œâ”€â”€ basic_chain.py       â†’ Learn fundamentals
â”‚   â”œâ”€â”€ conversation_app.py  â†’ Build chatbots
â”‚   â”œâ”€â”€ rag_app.py          â†’ Document Q&A
â”‚   â”œâ”€â”€ agent_app.py        â†’ Autonomous agents
â”‚   â””â”€â”€ streamlit_app.py    â†’ Web interface
â”‚
â””â”€â”€ Data Layer
    â”œâ”€â”€ data/               â†’ Documents
    â””â”€â”€ vector_stores/      â†’ Vector databases
```

### Key Components

**1. LLM (Large Language Model)**
- The "brain" that generates responses
- We use OpenAI's GPT models (gpt-3.5-turbo, gpt-4)

**2. Prompts**
- Templates that structure how we communicate with the LLM
- Include variables for dynamic content

**3. Chains**
- Connect multiple operations in sequence
- LLM + Prompt = Basic Chain
- Multiple chains = Sequential processing

**4. Memory**
- Stores conversation history
- Maintains context across messages

**5. Agents**
- Autonomous decision-makers
- Can use tools to accomplish tasks

**6. Vector Stores**
- Store document embeddings
- Enable semantic search

---

## 2. Basic Chain Application

### File: `basic_chain.py`

This is the **starting point** - it teaches you LangChain fundamentals through 5 progressive examples.

### Example 1: Simple LLM Call

**What it does:**
```python
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
response = llm.invoke("What is LangChain? Answer in one sentence.")
```

**Step by step:**

1. **Initialize the LLM** - Create a connection to OpenAI's model
   - `model`: Which GPT model to use
   - `temperature`: Controls randomness (0 = deterministic, 1 = creative)
   - `api_key`: Your OpenAI API key

2. **Invoke** - Send a question and get a response
   - Input: Plain text question
   - Output: AI-generated answer

**Output example:**
```
Response: LangChain is a framework for building applications powered by large language models through composable components.
```

**Key Learning:** Direct LLM interaction - the simplest form of AI communication.

---

### Example 2: Prompt Templates

**What it does:**
Creates reusable prompt structures with variables.

```python
template = """You are a helpful assistant that explains concepts simply.

Concept: {concept}
Audience: {audience}

Please explain the concept in a way that the audience will understand:"""

prompt = PromptTemplate(
    input_variables=["concept", "audience"],
    template=template
)

formatted = prompt.format(
    concept="Machine Learning",
    audience="a 10-year-old child"
)
```

**Step by step:**

1. **Define Template** - Create a text structure with placeholders `{variable_name}`
2. **Create PromptTemplate** - Specify which variables are needed
3. **Format** - Fill in the variables with actual values
4. **Use with LLM** - Send the formatted prompt to get a response

**Why this matters:**
- **Reusability**: Write once, use with different values
- **Consistency**: Same structure every time
- **Maintainability**: Change prompt in one place
- **Clarity**: Separate logic from data

**Output example:**
```
Machine Learning is like teaching a computer to learn from examples, just like how you learn from experience. Instead of telling the computer exactly what to do, we show it lots of examples and it figures out the patterns by itself!
```

---

### Example 3: LLMChain

**What it does:**
Combines an LLM with a prompt template into a reusable chain.

```python
prompt = PromptTemplate(
    input_variables=["topic", "style"],
    template="Write a {style} story about {topic}. Keep it under 100 words."
)

chain = LLMChain(llm=llm, prompt=prompt)

result = chain.invoke({
    "topic": "a robot learning to paint",
    "style": "humorous"
})
```

**Step by step:**

1. **Create Prompt** - Template with variables
2. **Create Chain** - LLMChain(llm + prompt)
3. **Invoke Chain** - Pass dictionary with variable values
4. **Get Result** - Receive generated text

**The Chain Pattern:**
```
Input Variables â†’ Prompt Template â†’ LLM â†’ Output
     â†“                 â†“              â†“        â†“
{"topic": "...",  "Write a...    GPT-3.5  "Story text"
 "style": "..."}   story..."
```

**Output example:**
```
Story: BeepBoop-3000 had finally discovered art. His first painting? A banana. Or was it a blueprint? Even he couldn't tell. As yellow paint dripped from his mechanical fingers onto the canvas (and the floor, and himself), he proclaimed, "I call it 'Systematic Fruit Chaos!'" His creator facepalmed. His AI friend said, "Still better than my poetry."
```

---

### Example 4: Sequential Chain

**What it does:**
Chains multiple steps together - output of one becomes input of next.

```python
# Chain 1: Generate product name
chain1 = LLMChain(llm=llm, prompt=prompt1)

# Chain 2: Generate tagline from product name
chain2 = LLMChain(llm=llm, prompt=prompt2)

# Combine them
overall_chain = SimpleSequentialChain(
    chains=[chain1, chain2],
    verbose=True
)

result = overall_chain.invoke("An AI app that helps learn languages")
```

**The Flow:**
```
Input: "AI app for language learning"
    â†“
Chain 1: Generate product name
    â†“
Output: "LinguaAI"
    â†“
Chain 2: Generate tagline
    â†“
Output: "Speak the world, one conversation at a time"
```

**Step by step:**

1. **Define Chain 1** - Takes product description â†’ Returns product name
2. **Define Chain 2** - Takes product name â†’ Returns tagline
3. **Combine Chains** - Chain them sequentially
4. **Execute** - Input flows through all chains

**When to use:**
- Multi-step processes (outline â†’ draft â†’ polish)
- Data transformation pipelines
- Content generation workflows

**Output example:**
```
> Entering new SimpleSequentialChain chain...
LinguaLeap

"Speak confidently, connect globally"

> Finished chain.
```

---

### Example 5: Chat Prompt Templates

**What it does:**
Creates structured conversation with system and user roles.

```python
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful coding assistant who explains concepts clearly."),
    ("human", "Explain {concept} in Python with a simple example."),
])

chain = chat_prompt | llm
result = chain.invoke({"concept": "list comprehension"})
```

**Message Roles:**

1. **System Message** - Sets the AI's behavior/personality
2. **Human Message** - User's question or request
3. **AI Message** - AI's response (in conversation history)

**The Pipe Operator `|`:**
```python
chain = chat_prompt | llm
# Equivalent to: take chat_prompt output and pass to llm
```

**Why use chat format?**
- Better context understanding
- Consistent AI behavior
- More natural conversations
- Separate instructions from questions

**Output example:**
```
Response:
List comprehension is a concise way to create lists in Python. Instead of writing a loop, you can create a new list in one line.

Example:
# Traditional loop
numbers = []
for i in range(5):
    numbers.append(i * 2)
# Result: [0, 2, 4, 6, 8]

# List comprehension
numbers = [i * 2 for i in range(5)]
# Result: [0, 2, 4, 6, 8]

The list comprehension is more readable and often faster!
```

---

## 3. Conversation with Memory

### File: `conversation_app.py`

**Purpose:** Build chatbots that remember previous messages.

### Memory Types

#### 1. Buffer Memory (Keeps Everything)

```python
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory)

# Conversation flow
User: "Hi! My name is Alice and I love Python."
AI: "Hello Alice! It's great to meet someone who loves Python..."

User: "What's my name?"
AI: "Your name is Alice."  âœ“ Remembers!

User: "What programming language do I like?"
AI: "You mentioned that you love Python."  âœ“ Remembers!
```

**How it works:**
- Stores ALL messages in memory
- Perfect recall of entire conversation
- **Pros:** Complete context
- **Cons:** Token usage grows (= more expensive)

---

#### 2. Window Memory (Last N Messages)

```python
memory = ConversationBufferWindowMemory(k=2)  # Keep last 2 exchanges

User: "My favorite color is blue."
AI: "That's a lovely choice..."

User: "I work as a data scientist."
AI: "Data science is an exciting field..."

User: "I have two cats."
AI: "Cats make wonderful pets..."

User: "What's my favorite color?"
AI: "I don't have that information..." âœ— Forgotten (too old)

User: "What's my occupation?"
AI: "You mentioned you work as a data scientist." âœ“ Recent enough
```

**How it works:**
- Keeps only the last N message pairs
- Older messages are dropped
- **Pros:** Fixed memory size, cost-effective
- **Cons:** Loses old context

**When to use:** Long conversations where recent context matters most

---

#### 3. Summary Memory (Summarizes History)

```python
memory = ConversationSummaryMemory(llm=llm)

User: "I'm planning a trip to Japan next month. Excited about Tokyo and Kyoto."
AI: "Japan is wonderful! Tokyo offers modern culture..."

User: "I love sushi and ramen. Can't wait for authentic Japanese food!"
AI: "You're in for a treat! The food in Japan..."

User: "Where am I planning to travel?"
AI: "Based on our conversation, you're planning to travel to Japan..." âœ“
```

**How it works:**
- Summarizes old messages to save tokens
- Keeps full recent messages
- **Pros:** Maintains key info, efficient
- **Cons:** May lose details

---

### Custom Conversation Contexts

```python
template = """You are a friendly Python programming tutor.
You help students learn Python by providing clear explanations.

Current conversation:
{history}
Human: {input}
AI Assistant:"""

conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=PromptTemplate(template=template)
)
```

**Customization benefits:**
- Specific personality/role
- Consistent behavior
- Domain expertise
- Custom formatting

---

## 4. RAG (Document Q&A)

### File: `rag_app.py`

**Purpose:** Answer questions based on your own documents.

### The RAG Pipeline

```
1. Load Documents
   â†“
2. Split into Chunks
   â†“
3. Create Embeddings (Vector representations)
   â†“
4. Store in Vector Database
   â†“
5. User asks question
   â†“
6. Find relevant chunks (Retrieval)
   â†“
7. Send to LLM with context (Augmented Generation)
   â†“
8. Get answer with sources
```

### Step-by-Step Breakdown

#### Step 1: Load Documents

```python
loader = DirectoryLoader(
    "data/",
    glob="**/*.txt",
    loader_cls=TextLoader
)
documents = loader.load()
```

**What happens:**
- Scans `data/` folder for `.txt` files
- Loads each file as a Document object
- Each Document has: `page_content` (text) and `metadata` (file info)

---

#### Step 2: Split into Chunks

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Max characters per chunk
    chunk_overlap=200,    # Overlap between chunks
)
chunks = text_splitter.split_documents(documents)
```

**Why split?**
- LLMs have token limits
- Smaller chunks = more precise retrieval
- Overlap prevents context loss at boundaries

**Example:**
```
Document: "Python is great. Python has many libraries. Django is a web framework..."

Chunk 1: "Python is great. Python has many libraries..." (1000 chars)
Chunk 2: "...many libraries. Django is a web framework..." (1000 chars, 200 overlap)
```

---

#### Step 3: Create Embeddings

```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

**What are embeddings?**
- Vector representations of text
- Similar meanings â†’ Similar vectors
- Enables semantic search

**Example:**
```
"Python programming" â†’ [0.2, 0.8, 0.1, ...]
"Python coding"      â†’ [0.3, 0.7, 0.2, ...]  (Similar!)
"Banana recipe"      â†’ [0.9, 0.1, 0.8, ...]  (Different!)
```

---

#### Step 4: Vector Store

```python
vector_store = FAISS.from_documents(chunks, embeddings)
```

**What it does:**
- Stores chunk embeddings
- Enables fast similarity search
- **FAISS**: Facebook AI Similarity Search (super fast)

---

#### Step 5-7: Retrieval & Generation

```python
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

result = qa_chain.invoke({"query": "What are Python's key features?"})
```

**The Magic:**

1. **User asks:** "What are Python's key features?"
2. **Embedding created:** `[0.1, 0.7, ...]` for the question
3. **Similarity search:** Find 3 most similar chunks
4. **Context building:**
   ```
   Context: [Chunk 1] [Chunk 2] [Chunk 3]
   Question: What are Python's key features?
   ```
5. **LLM generates answer** using the context
6. **Return answer + sources** so you can verify

**Output:**
```
Answer:
Python's key features include:
1. Easy to learn and read
2. Versatile for web, data science, AI
3. Large standard library
4. Active community

Sources:
1. data/python_basics.txt
   "Python is a high-level language known for simplicity..."
```

---

### Why RAG is Powerful

**Without RAG:**
- LLM only knows training data (outdated)
- Can't access your private documents
- May hallucinate

**With RAG:**
- âœ“ Uses YOUR documents
- âœ“ Always up-to-date (just update docs)
- âœ“ Cites sources (verifiable)
- âœ“ No hallucination (answers from docs)

---

## 5. Agent with Tools

### File: `agent_app.py`

**Purpose:** AI that can USE TOOLS to accomplish tasks autonomously.

### The ReAct Pattern

```
Thought â†’ Action â†’ Observation â†’ Thought â†’ ... â†’ Answer
```

### Example: "Calculate 15% tip on $85"

**Agent's thought process:**
```
1. Thought: "I need to calculate 15% of 85"
2. Action: calculator("85 * 0.15")
3. Observation: "12.75"
4. Thought: "The tip is $12.75. Total would be $85 + $12.75"
5. Action: calculator("85 + 12.75")
6. Observation: "97.75"
7. Thought: "I now have the complete answer"
8. Final Answer: "A 15% tip on $85 is $12.75, making the total $97.75"
```

### Creating Custom Tools

```python
@tool
def calculate(expression: str) -> str:
    """
    Evaluate a mathematical expression and return the result.
    Use this for any math calculations.
    """
    try:
        result = eval(expression)
        return str(result)
    except Exception as e:
        return f"Error: {str(e)}"
```

**Key components:**
- `@tool` decorator - marks function as a tool
- **Docstring** - CRITICAL! Agent reads this to know when to use it
- Clear **name** and **description**
- Error handling

### Available Tools in Our App

1. **get_current_time** - Returns current date/time
2. **calculate** - Math operations
3. **word_counter** - Counts words in text
4. **text_reverser** - Reverses text
5. **create_file** - Creates files
6. **read_file** - Reads files

### Agent Creation

```python
agent = create_react_agent(llm, tools, prompt)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,              # Shows thinking process
    max_iterations=10,         # Safety limit
    handle_parsing_errors=True # Graceful error handling
)
```

### Example Execution

**Task:** "Create a file called 'greeting.txt' with the text 'Hello World' and then read it back"

**Agent process:**
```
> Entering new AgentExecutor chain...

Thought: I need to create a file first, then read it
Action: create_file
Action Input: filename='greeting.txt', content='Hello World'
Observation: âœ… File created successfully: /path/to/greeting.txt

Thought: Now I need to read the file to verify
Action: read_file
Action Input: filename='greeting.txt'
Observation: File contents: Hello World

Thought: I have successfully completed the task
Final Answer: I created 'greeting.txt' with "Hello World" and verified it contains the correct text.

> Finished chain.
```

### When to Use Agents

âœ“ **Good for:**
- Multi-step tasks
- Tasks requiring external tools
- Dynamic decision-making
- Exploratory problems

âœ— **Not ideal for:**
- Simple single-step tasks (use chains)
- Highly deterministic flows (use sequential chains)
- Cost-sensitive applications (agents make multiple LLM calls)

---

## 6. Web UI

### File: `streamlit_app.py`

**Purpose:** User-friendly interface - no code required!

### Features

#### 1. Simple Chat
- Conversational interface
- Memory persists during session
- Real-time responses

#### 2. Prompt Playground
- Test different prompts
- Experiment with variables
- See results instantly

#### 3. Text Analysis
- Summarize text
- Extract key points
- Rephrase content

### Running the UI

```bash
streamlit run streamlit_app.py
```

**What happens:**
1. Streamlit starts local server
2. Opens browser automatically
3. Live-reloading on code changes

### UI Architecture

```python
# Session state for persistence
if 'messages' not in st.session_state:
    st.session_state.messages = []

# Sidebar for config
with st.sidebar:
    api_key = st.text_input("API Key", type="password")
    model = st.selectbox("Model", ["gpt-3.5-turbo", "gpt-4"])

# Chat interface
if prompt := st.chat_input("Your message"):
    # Get AI response
    response = conversation.predict(input=prompt)
    st.write(response)
```

---

## Complete Workflow Example

### Building a Document Q&A System

**Scenario:** You have company documents and want employees to ask questions.

#### Step 1: Prepare Documents

```bash
# Add your documents to data/ folder
day10/data/
  â”œâ”€â”€ company_policies.txt
  â”œâ”€â”€ employee_handbook.txt
  â””â”€â”€ benefits_guide.txt
```

#### Step 2: Run RAG App

```bash
python rag_app.py
```

#### Step 3: Ask Questions

```
You: "What is our vacation policy?"

AI: "According to the company policies, employees receive:
- 15 days vacation per year
- 10 sick days
- 8 federal holidays

Source: data/company_policies.txt"
```

---

## Key Takeaways

### 1. LangChain Abstractions

| Component | Purpose | When to Use |
|-----------|---------|-------------|
| **Prompts** | Structure AI instructions | Always |
| **Chains** | Sequential operations | Multi-step tasks |
| **Memory** | Remember context | Conversations |
| **Agents** | Autonomous tool use | Complex tasks |
| **RAG** | Use custom documents | Knowledge bases |

### 2. Best Practices

âœ“ **Do:**
- Start simple (basic chains)
- Use prompt templates for reusability
- Test with low-cost models first (gpt-3.5-turbo)
- Handle errors gracefully
- Set max_tokens limits

âœ— **Don't:**
- Hardcode prompts
- Forget error handling
- Skip the verbose mode during development
- Use agents for simple tasks
- Expose API keys in code

### 3. Cost Optimization

**Expensive:**
- GPT-4 with long conversations
- Agents (multiple LLM calls)
- Large embeddings

**Cheaper:**
- GPT-3.5-turbo
- Window memory (limited history)
- Local embeddings (HuggingFace)
- Caching results

### 4. Development Flow

```
1. Prototype with basic_chain.py
   â†“
2. Add memory (conversation_app.py)
   â†“
3. Add documents (rag_app.py)
   â†“
4. Add tools (agent_app.py)
   â†“
5. Build UI (streamlit_app.py)
```

---

## Next Steps

### Beginner
- [x] Run all basic examples
- [ ] Modify prompts and see results
- [ ] Try different temperatures
- [ ] Add your own documents to RAG

### Intermediate
- [ ] Create custom tools for agent
- [ ] Build a specialized chatbot
- [ ] Implement conversation summary memory
- [ ] Deploy Streamlit app

### Advanced
- [ ] Combine RAG + Agent
- [ ] Add web search tools
- [ ] Implement streaming responses
- [ ] Build production app with authentication
- [ ] Fine-tune prompts for your domain

---

## Troubleshooting

### Common Issues

**Issue:** "Module not found"
```bash
Solution: pip install -r requirements.txt
```

**Issue:** "API key not found"
```bash
Solution: Create .env file with OPENAI_API_KEY=your_key
```

**Issue:** "Rate limit exceeded"
```bash
Solution: 
1. Check OpenAI usage dashboard
2. Add credits
3. Use smaller models
4. Reduce frequency
```

**Issue:** "Context length exceeded"
```bash
Solution:
1. Use window memory
2. Reduce chunk_size in RAG
3. Limit retrieval results (k=3 instead of k=10)
```

---

## Resources

- **This Project:** Start with `QUICKSTART.md`
- **LangChain Docs:** https://python.langchain.com/
- **OpenAI Cookbook:** https://cookbook.openai.com/
- **Prompt Engineering:** https://www.promptingguide.ai/

---

## Congratulations! ðŸŽ‰

You now understand:
- âœ“ LangChain fundamentals
- âœ“ Prompt engineering
- âœ“ Memory management
- âœ“ RAG implementation
- âœ“ Agent patterns
- âœ“ Production deployment

**Keep experimenting and building!** ðŸš€

